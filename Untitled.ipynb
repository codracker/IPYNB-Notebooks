{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a,b,c,d\\n\\ne,f,g,h']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sample.txt') as f:\n",
    "    lines = f.read()\n",
    "\n",
    "tests = lines.split(\"/n/n\")\n",
    "tests\n",
    "\n",
    "\n",
    "with open(\"sample.txt\", \"r\") as input:\n",
    "    input_ = input.read().split(\"\\n\\n\")\n",
    "    print(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object txt2paragraph at 0x0000012C78F0E9E0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def txt2paragraph(filepath):\n",
    "    with open(filepath) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    paragraph = ''\n",
    "    for line in lines:\n",
    "        if line.isspace():  # is it an empty line?\n",
    "            if paragraph:\n",
    "                yield paragraph\n",
    "                paragraph = ''\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            paragraph += ' ' + line.strip()\n",
    "    yield paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-0ef9c2a23440>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-0ef9c2a23440>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    puts \"#{paragraph_count} paragraphs.\"\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "lines = File.readlines(\"sample.txt\")\n",
    "line_count = lines.size\n",
    "text = lines.join\n",
    "paragraph_count = text.split(\"\\.\\r\").length\n",
    "puts \"#{paragraph_count} paragraphs.\"\n",
    "\n",
    "testArray = Array.new(paragraph_count.to_i, Hash.new)\n",
    "for i in 0...paragraph_count.to_i do\n",
    "    testArray[i] = Hash.new \n",
    "    puts \"testArray #{i} has been made\"\n",
    "end\n",
    "words_in_each_paragraph = Array.new\n",
    "\n",
    "File.foreach(\"test.txt\", \"\\.\\r\") do |paragraph|\n",
    "    word_hash = {}\n",
    "    paragraph.split(/\\W+/).each_with_object(word_hash) { |w, h|\n",
    "        h[w] = []\n",
    "    }\n",
    "    words_in_each_paragraph << word_hash\n",
    "    testArray[i][:value] = word_hash\n",
    "    puts testArray[i] # IT WORKS HERE #\n",
    "end\n",
    "\n",
    "puts testArray[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through\\n the analysis of large corpora (the plural form of corpus, is a set of documents, shri. possibly with human or computer annotations) \\nof typical real-world examples.\\n'], [], []]\n",
      "[['Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through\\n the analysis of large corpora (the plural form of corpus, is a set of documents, shri. possibly with human or computer annotations) \\nof typical real-world examples.\\n'], ['Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks.\\n These algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, st.\\nhowever, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights\\n to each input feature. Such models have the advantage that they can express the relative certainty of many different possible\\n answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\\n'], []]\n",
      "[['Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through\\n the analysis of large corpora (the plural form of corpus, is a set of documents, shri. possibly with human or computer annotations) \\nof typical real-world examples.\\n'], ['Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks.\\n These algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, st.\\nhowever, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights\\n to each input feature. Such models have the advantage that they can express the relative certainty of many different possible\\n answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\\n'], ['Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing\\n hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly,\\n research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features\\n making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.\\n']]\n",
      "[['Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through\\n the analysis of large corpora (the plural form of corpus, is a set of documents, shri. possibly with human or computer annotations) \\nof typical real-world examples.\\n', 'Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data),\\n and produce more reliable results when integrated into a larger system comprising multiple subtasks, Ms. \\n'], ['Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks.\\n These algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, st.\\nhowever, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights\\n to each input feature. Such models have the advantage that they can express the relative certainty of many different possible\\n answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\\n'], ['Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing\\n hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly,\\n research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features\\n making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.\\n']]\n",
      "[['Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through\\n the analysis of large corpora (the plural form of corpus, is a set of documents, shri. possibly with human or computer annotations) \\nof typical real-world examples.\\n', 'Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data),\\n and produce more reliable results when integrated into a larger system comprising multiple subtasks, Ms. \\n'], ['Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks.\\n These algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, st.\\nhowever, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights\\n to each input feature. Such models have the advantage that they can express the relative certainty of many different possible\\n answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\\n', 'Since the neural turn, statistical methods in NLP research have been largely replaced by neural networks.\\n'], ['Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing\\n hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly,\\n research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features\\n making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.\\n']]\n",
      "[['Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through\\n the analysis of large corpora (the plural form of corpus, is a set of documents, shri. possibly with human or computer annotations) \\nof typical real-world examples.\\n', 'Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data),\\n and produce more reliable results when integrated into a larger system comprising multiple subtasks, Ms. \\n'], ['Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks.\\n These algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, st.\\nhowever, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights\\n to each input feature. Such models have the advantage that they can express the relative certainty of many different possible\\n answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\\n', 'Since the neural turn, statistical methods in NLP research have been largely replaced by neural networks.\\n'], ['Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing\\n hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly,\\n research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features\\n making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.\\n', '\\n']]\n",
      "[['Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through\\n the analysis of large corpora (the plural form of corpus, is a set of documents, shri. possibly with human or computer annotations) \\nof typical real-world examples.\\n', 'Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data),\\n and produce more reliable results when integrated into a larger system comprising multiple subtasks, Ms. \\n', ' However, they continue to be relevant for contexts in which statistical interpretability and transparency is required.\\nMr.Xyzz who is r/o of wqufwbf,lane 1, baiufqb \\n'], ['Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks.\\n These algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, st.\\nhowever, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights\\n to each input feature. Such models have the advantage that they can express the relative certainty of many different possible\\n answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\\n', 'Since the neural turn, statistical methods in NLP research have been largely replaced by neural networks.\\n'], ['Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing\\n hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly,\\n research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features\\n making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.\\n', '\\n']]\n",
      "[['Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through\\n the analysis of large corpora (the plural form of corpus, is a set of documents, shri. possibly with human or computer annotations) \\nof typical real-world examples.\\n', 'Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data),\\n and produce more reliable results when integrated into a larger system comprising multiple subtasks, Ms. \\n', ' However, they continue to be relevant for contexts in which statistical interpretability and transparency is required.\\nMr.Xyzz who is r/o of wqufwbf,lane 1, baiufqb \\n'], ['Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks.\\n These algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, st.\\nhowever, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights\\n to each input feature. Such models have the advantage that they can express the relative certainty of many different possible\\n answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\\n', 'Since the neural turn, statistical methods in NLP research have been largely replaced by neural networks.\\n', ''], ['Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing\\n hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly,\\n research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features\\n making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.\\n', '\\n']]\n"
     ]
    }
   ],
   "source": [
    "import itertools as it\n",
    "\n",
    "\n",
    "def paragraphs(fileobj, separator='\\n'):\n",
    "    \"\"\"Iterate a fileobject by paragraph\"\"\"\n",
    "    ## Makes no assumptions about the encoding used in the file\n",
    "    lines = []\n",
    "    for line in fileobj:\n",
    "        if line == separator and lines:\n",
    "            yield ''.join(lines)\n",
    "            lines = []\n",
    "        else:\n",
    "            lines.append(line)\n",
    "    yield ''.join(lines)\n",
    "\n",
    "paragraph_lists = [[], [], []]\n",
    "with open('sample.txt') as f:\n",
    "    paras = paragraphs(f)\n",
    "    for para, group in zip(paras, it.cycle(paragraph_lists)):\n",
    "        group.append(para)\n",
    "\n",
    "        print(paragraph_lists)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
